{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP+sYDVh8z+XEKP9+OCLENN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seonhe/PyTorch/blob/master/Untitled11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TxOLrgxLhhWS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function, Variable\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "#for reproducibility\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "# CIFAR10\n",
        "def get_loaders(batch_size):\n",
        "    train_loader = torch.utils.data.DataLoader(datasets.CIFAR10('/content/CIFAR', train=True, download = True,\n",
        "                                                              transform = transforms.Compose([\n",
        "                                                                  transforms.ToTensor(),\n",
        "                                                                  transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))  # R, G, B\n",
        "                                                ])), batch_size = params['batch_size'], shuffle = True)\n",
        "    test_loader = torch.utils.data.DataLoader(datasets.CIFAR10('/content/CIFAR', train=False, download = True,\n",
        "                                                              transform = transforms.Compose([\n",
        "                                                                  transforms.ToTensor(),\n",
        "                                                                  transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "                                                ])), batch_size = params['batch_size'], shuffle = True)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "class XNORModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #self.BinConv2d_1 = BinConv2d(3, 128,  kernel_size=1).to('cuda')\n",
        "        self.Conv2d_1 = nn.Sequential(nn.Conv2d(3,128,kernel_size=3,stride=1,padding=0).to(device), nn.BatchNorm2d(128,affine=False).to(device), nn.ReLU().to(device))\n",
        "        self.BinConv2d_2 = BinConv2d(128, 128, kernel_size=3).to(device)\n",
        "        self.BinConv2d_3 = BinConv2d(128, 256, kernel_size=3).to(device)\n",
        "        self.BinConv2d_4 = BinConv2d(256, 256, kernel_size=3).to(device)\n",
        "        self.BinConv2d_5 = BinConv2d(256, 512, kernel_size=3).to(device)\n",
        "        self.BinConv2d_6 = BinConv2d(512, 512, kernel_size=3).to(device)\n",
        "        \n",
        "        self.pool        = nn.MaxPool2d(kernel_size=2, stride=2).to(device)\n",
        "        \n",
        "        self.fc1         = BinLinear(51200, 1024).to(device)\n",
        "        self.fc2         = BinLinear(1024, 512).to(device)\n",
        "        self.fc3         = BinLinear(512, 10).to(device)\n",
        "\n",
        "    def forward(self, I):\n",
        "        I = I.to('cuda')\n",
        "        I = self.Conv2d_1(I) #30\n",
        "        #I = self.BinConv2d_1(I) #32\n",
        "        I = self.BinConv2d_2(I) #28\n",
        "        I = self.BinConv2d_3(I) #26\n",
        "        #I = self.pool(I) #14\n",
        "        \n",
        "        I = self.BinConv2d_4(I) #24\n",
        "        I = self.BinConv2d_5(I) #22\n",
        "        I = self.BinConv2d_6(I) #20\n",
        "        I = self.pool(I) #10\n",
        "\n",
        "        I = I.view(-1, 51200)\n",
        "        I = self.fc1(I)\n",
        "        I = F.relu(I)\n",
        "        I = self.fc2(I) \n",
        "        I = F.relu(I)\n",
        "        I = self.fc3(I)\n",
        "        \n",
        "        return I\n",
        "\n",
        "\n",
        "class BinConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "        self.in_channels  = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size  = kernel_size\n",
        "        self.stride       = stride\n",
        "        self.padding      = padding\n",
        "        self.bn           = nn.BatchNorm2d(in_channels) # default eps = 1e-5, momentum = 0.1, affine = True\n",
        "        self.conv         = nn.Conv2d(in_channels, out_channels,kernel_size=kernel_size)\n",
        "        self.relu         = nn.ReLU()\n",
        "        self.pool         = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, I):\n",
        "        I = self.bn(I)\n",
        "        A = BinActiv().Mean(I)\n",
        "        I = BinActive(I)\n",
        "        k = torch.ones(1,1,self.kernel_size,self.kernel_size).mul(1/(self.kernel_size**2)).to('cuda') # 4d - batch,channel,height,width\n",
        "        K = F.conv2d(A,k).to(device) # default stride=1, padding=0\n",
        "        I = self.conv(I)\n",
        "        I = torch.mul(I, K)\n",
        "        I = self.relu(I)\n",
        "        #I = self.pool(I)\n",
        "\n",
        "        return I\n",
        "\n",
        "\n",
        "\n",
        "class BinActiv(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        input = torch.sign(input)\n",
        "\n",
        "        return input\n",
        "\n",
        "    def Mean(self, input):\n",
        "        return torch.mean(input.abs(), 1, keepdim=True)  # 1: channel // batch[0], channel[1], height[2], width[3]\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input,  = ctx.saved_tensors\n",
        "\n",
        "        # STE (Straight Through Estimator)\n",
        "        grad_input = grad_output.clone()\n",
        "        grad_input[input.ge(1)] = 0    # ge: greater or equal\n",
        "        grad_input[input.le(-1)] = 0   # le: less or equal\n",
        "        return grad_input\n",
        "\n",
        "BinActive = BinActiv.apply\n",
        "\n",
        "class BinLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.in_feature  = in_features\n",
        "        self.out_feature = out_features\n",
        "        self.bn          = nn.BatchNorm1d(in_features)\n",
        "        self.linear      = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, I):\n",
        "        I = self.bn(I)\n",
        "        beta = BinActiv().Mean(I).expand_as(I)\n",
        "        I = BinActive(I)\n",
        "        I = torch.mul(I, beta)\n",
        "        I = self.linear(I)\n",
        "        return I\n",
        "\n",
        "\n",
        "\n",
        "class WeightOperation:\n",
        "    def __init__(self,model):\n",
        "\n",
        "        self.count_group_weights = 0\n",
        "        self.weight = []\n",
        "        self.saved_weight = []\n",
        "\n",
        "\n",
        "        for m in model.modules():\n",
        "            if isinstance(m,nn.Conv2d) or isinstance(m,nn.Linear):\n",
        "\n",
        "                self.count_group_weights += 1\n",
        "                self.weight.append(m.weight)\n",
        "                self.saved_weight.append(m.weight.data)\n",
        "\n",
        "\n",
        "    def WeightSave(self):\n",
        "        for index in range(self.count_group_weights):\n",
        "            self.saved_weight[index].copy_(self.weight[index].data)\n",
        "\n",
        "\n",
        "    def WeightBinarize(self):\n",
        "        for index in range(self.count_group_weights):\n",
        "\n",
        "            n                 = self.weight[index].data[0].nelement()\n",
        "            dim_group_weights = self.weight[index].data.size()\n",
        "\n",
        "            if len(dim_group_weights) == 4:\n",
        "                alpha = self.weight[index].data.norm(1, 3, keepdim=True).sum(2, keepdim=True).sum(1, keepdim=True).div(n).expand(dim_group_weights)\n",
        "\n",
        "            elif len(dim_group_weights) == 2:\n",
        "                alpha = self.weight[index].data.norm(1, 1, keepdim=True).div(n).expand(dim_group_weights)\n",
        "\n",
        "            self.weight[index].data = self.weight[index].data.sign()* alpha\n",
        "\n",
        "\n",
        "    def WeightRestore(self):\n",
        "        for index in range(self.count_group_weights):\n",
        "            self.weight[index].data.copy_(self.saved_weight[index])\n",
        "\n",
        "\n",
        "    def WeightGradient(self):\n",
        "        for index in range(self.count_group_weights):\n",
        "            n = self.weight[index].data[0].nelement()\n",
        "            dim_group_weights = self.weight[index].data.size()\n",
        "\n",
        "            if len(dim_group_weights) == 4:\n",
        "                alpha = self.weight[index].data.norm(1, 3, keepdim=True).sum(2, keepdim=True).sum(1, keepdim=True).div(n).expand(dim_group_weights)\n",
        "\n",
        "            elif len(dim_group_weights) == 2:\n",
        "                alpha = self.weight[index].data.norm(1, 1, keepdim=True).div(n).expand(dim_group_weights)\n",
        "\n",
        "            alpha[self.weight[index].data.le(-1.0)] = 0\n",
        "            alpha[self.weight[index].data.ge( 1.0)] = 0\n",
        "\n",
        "            self.weight[index].grad = self.weight[index].grad / n + self.weight[index].grad * alpha"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = XNORModel()\n",
        "model = XNORModel().to(device)\n",
        "\n",
        "WeightOperation = WeightOperation(model)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.0003)\n",
        "lr_sche=optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.8)\n",
        "params = {'epochs':30, 'batch_size':100}\n",
        "loss_fn   = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "train_loader, test_loader = get_loaders(batch_size=params['batch_size'])\n",
        "\n",
        "for epoch in range(10):\n",
        "    lr_sche.step()\n",
        "    # training\n",
        "    for batch_idx, (train_inputs, train_labels) in enumerate(train_loader): # train_inputs size:[32,1,28,28], labels size: [32]\n",
        "        train_inputs = train_inputs.to(device)\n",
        "        train_labels = train_labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        WeightOperation.WeightSave()\n",
        "        WeightOperation.WeightBinarize()\n",
        "\n",
        "        predicted = model(train_inputs)\n",
        "        loss = loss_fn(predicted, train_labels)\n",
        "        loss.backward()     # gradient\n",
        "\n",
        "        WeightOperation.WeightRestore()\n",
        "        WeightOperation.WeightGradient()\n",
        "\n",
        "        optimizer.step()    # update\n",
        "      \n",
        "\n",
        "        if((batch_idx*len(train_inputs))%1000==0):\n",
        "           print('[%d, %5d] loss: %.3f' %(epoch, batch_idx*len(train_inputs), loss.item()))  # loss: loss tensor(2.3027, grad_fn=<NllLossBackward>)\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lTuhMHwphpbJ",
        "outputId": "412f6c8d-2c71-4e82-b9d5-0aeb5a120b22"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: UserWarning: Use of index_put_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[indices] = tensor (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:515.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: UserWarning: Use of masked_fill_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[mask] = scalar (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:946.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:196: UserWarning: Use of index_put_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[indices] = tensor (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:515.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:196: UserWarning: Use of masked_fill_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[mask] = scalar (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:946.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0,     0] loss: 2.413\n",
            "[0,  1000] loss: 1.965\n",
            "[0,  2000] loss: 1.951\n",
            "[0,  3000] loss: 1.763\n",
            "[0,  4000] loss: 1.805\n",
            "[0,  5000] loss: 1.699\n",
            "[0,  6000] loss: 1.958\n",
            "[0,  7000] loss: 1.821\n",
            "[0,  8000] loss: 1.509\n",
            "[0,  9000] loss: 1.746\n",
            "[0, 10000] loss: 1.591\n",
            "[0, 11000] loss: 1.389\n",
            "[0, 12000] loss: 1.622\n",
            "[0, 13000] loss: 1.509\n",
            "[0, 14000] loss: 1.735\n",
            "[0, 15000] loss: 1.555\n",
            "[0, 16000] loss: 1.515\n",
            "[0, 17000] loss: 1.400\n",
            "[0, 18000] loss: 1.555\n",
            "[0, 19000] loss: 1.510\n",
            "[0, 20000] loss: 1.442\n",
            "[0, 21000] loss: 1.257\n",
            "[0, 22000] loss: 1.421\n",
            "[0, 23000] loss: 1.559\n",
            "[0, 24000] loss: 1.679\n",
            "[0, 25000] loss: 1.300\n",
            "[0, 26000] loss: 1.484\n",
            "[0, 27000] loss: 1.412\n",
            "[0, 28000] loss: 1.430\n",
            "[0, 29000] loss: 1.485\n",
            "[0, 30000] loss: 1.535\n",
            "[0, 31000] loss: 1.257\n",
            "[0, 32000] loss: 1.665\n",
            "[0, 33000] loss: 1.311\n",
            "[0, 34000] loss: 1.318\n",
            "[0, 35000] loss: 1.240\n",
            "[0, 36000] loss: 1.389\n",
            "[0, 37000] loss: 1.364\n",
            "[0, 38000] loss: 1.378\n",
            "[0, 39000] loss: 1.479\n",
            "[0, 40000] loss: 1.112\n",
            "[0, 41000] loss: 1.428\n",
            "[0, 42000] loss: 1.410\n",
            "[0, 43000] loss: 1.399\n",
            "[0, 44000] loss: 1.266\n",
            "[0, 45000] loss: 1.370\n",
            "[0, 46000] loss: 1.161\n",
            "[0, 47000] loss: 1.148\n",
            "[0, 48000] loss: 1.225\n",
            "[0, 49000] loss: 1.270\n",
            "[1,     0] loss: 1.272\n",
            "[1,  1000] loss: 1.072\n",
            "[1,  2000] loss: 1.036\n",
            "[1,  3000] loss: 1.264\n",
            "[1,  4000] loss: 1.351\n",
            "[1,  5000] loss: 1.206\n",
            "[1,  6000] loss: 1.183\n",
            "[1,  7000] loss: 0.997\n",
            "[1,  8000] loss: 1.146\n",
            "[1,  9000] loss: 1.112\n",
            "[1, 10000] loss: 1.194\n",
            "[1, 11000] loss: 1.035\n",
            "[1, 12000] loss: 1.047\n",
            "[1, 13000] loss: 1.289\n",
            "[1, 14000] loss: 0.915\n",
            "[1, 15000] loss: 1.041\n",
            "[1, 16000] loss: 1.299\n",
            "[1, 17000] loss: 1.116\n",
            "[1, 18000] loss: 1.005\n",
            "[1, 19000] loss: 1.013\n",
            "[1, 20000] loss: 1.018\n",
            "[1, 21000] loss: 1.034\n",
            "[1, 22000] loss: 1.304\n",
            "[1, 23000] loss: 1.116\n",
            "[1, 24000] loss: 1.076\n",
            "[1, 25000] loss: 1.274\n",
            "[1, 26000] loss: 1.110\n",
            "[1, 27000] loss: 1.023\n",
            "[1, 28000] loss: 0.890\n",
            "[1, 29000] loss: 1.328\n",
            "[1, 30000] loss: 1.054\n",
            "[1, 31000] loss: 1.317\n",
            "[1, 32000] loss: 1.169\n",
            "[1, 33000] loss: 1.195\n",
            "[1, 34000] loss: 1.193\n",
            "[1, 35000] loss: 0.846\n",
            "[1, 36000] loss: 0.961\n",
            "[1, 37000] loss: 1.007\n",
            "[1, 38000] loss: 0.860\n",
            "[1, 39000] loss: 1.033\n",
            "[1, 40000] loss: 1.219\n",
            "[1, 41000] loss: 1.073\n",
            "[1, 42000] loss: 0.834\n",
            "[1, 43000] loss: 1.074\n",
            "[1, 44000] loss: 1.039\n",
            "[1, 45000] loss: 0.871\n",
            "[1, 46000] loss: 0.983\n",
            "[1, 47000] loss: 0.831\n",
            "[1, 48000] loss: 0.980\n",
            "[1, 49000] loss: 0.917\n",
            "[2,     0] loss: 1.171\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-cb9d1905dfc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# train_inputs size:[32,1,28,28], labels size: [32]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2):\n",
        "    lr_sche.step()\n",
        "    # training\n",
        "    for batch_idx, (train_inputs, train_labels) in enumerate(train_loader): # train_inputs size:[32,1,28,28], labels size: [32]\n",
        "        train_inputs = train_inputs.to(device)\n",
        "        train_labels = train_labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        WeightOperation.WeightSave()\n",
        "        WeightOperation.WeightBinarize()\n",
        "\n",
        "        predicted = model(train_inputs)\n",
        "        loss = loss_fn(predicted, train_labels)\n",
        "        loss.backward()     # gradient\n",
        "\n",
        "        WeightOperation.WeightRestore()\n",
        "        WeightOperation.WeightGradient()\n",
        "\n",
        "        optimizer.step()    # update\n",
        "      \n",
        "\n",
        "        if((batch_idx*len(train_inputs))%1000==0):\n",
        "           print('[%d, %5d] loss: %.3f' %(epoch, batch_idx*len(train_inputs), loss.item()))  # loss: loss tensor(2.3027, grad_fn=<NllLossBackward>)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrCOj-vfSLK9",
        "outputId": "d6c2c57d-0039-48a3-99c7-ff7ad727e41e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: UserWarning: Use of index_put_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[indices] = tensor (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:515.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: UserWarning: Use of masked_fill_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[mask] = scalar (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:946.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:196: UserWarning: Use of index_put_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[indices] = tensor (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:515.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:196: UserWarning: Use of masked_fill_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[mask] = scalar (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:946.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0,     0] loss: 0.879\n",
            "[0,  1000] loss: 1.092\n",
            "[0,  2000] loss: 0.917\n",
            "[0,  3000] loss: 0.767\n",
            "[0,  4000] loss: 0.925\n",
            "[0,  5000] loss: 0.985\n",
            "[0,  6000] loss: 0.851\n",
            "[0,  7000] loss: 0.850\n",
            "[0,  8000] loss: 0.983\n",
            "[0,  9000] loss: 0.970\n",
            "[0, 10000] loss: 0.799\n",
            "[0, 11000] loss: 0.871\n",
            "[0, 12000] loss: 0.932\n",
            "[0, 13000] loss: 0.805\n",
            "[0, 14000] loss: 0.990\n",
            "[0, 15000] loss: 0.902\n",
            "[0, 16000] loss: 0.891\n",
            "[0, 17000] loss: 0.770\n",
            "[0, 18000] loss: 0.948\n",
            "[0, 19000] loss: 0.965\n",
            "[0, 20000] loss: 0.873\n",
            "[0, 21000] loss: 0.801\n",
            "[0, 22000] loss: 1.074\n",
            "[0, 23000] loss: 0.990\n",
            "[0, 24000] loss: 0.918\n",
            "[0, 25000] loss: 0.936\n",
            "[0, 26000] loss: 0.801\n",
            "[0, 27000] loss: 0.966\n",
            "[0, 28000] loss: 0.849\n",
            "[0, 29000] loss: 0.751\n",
            "[0, 30000] loss: 0.900\n",
            "[0, 31000] loss: 0.780\n",
            "[0, 32000] loss: 0.914\n",
            "[0, 33000] loss: 0.988\n",
            "[0, 34000] loss: 0.909\n",
            "[0, 35000] loss: 0.800\n",
            "[0, 36000] loss: 0.795\n",
            "[0, 37000] loss: 0.958\n",
            "[0, 38000] loss: 0.678\n",
            "[0, 39000] loss: 0.829\n",
            "[0, 40000] loss: 0.923\n",
            "[0, 41000] loss: 0.744\n",
            "[0, 42000] loss: 0.857\n",
            "[0, 43000] loss: 0.902\n",
            "[0, 44000] loss: 0.817\n",
            "[0, 45000] loss: 1.009\n",
            "[0, 46000] loss: 1.021\n",
            "[0, 47000] loss: 0.751\n",
            "[0, 48000] loss: 0.962\n",
            "[0, 49000] loss: 0.689\n",
            "[1,     0] loss: 0.726\n",
            "[1,  1000] loss: 0.643\n",
            "[1,  2000] loss: 0.673\n",
            "[1,  3000] loss: 0.708\n",
            "[1,  4000] loss: 0.762\n",
            "[1,  5000] loss: 0.788\n",
            "[1,  6000] loss: 0.633\n",
            "[1,  7000] loss: 0.555\n",
            "[1,  8000] loss: 0.717\n",
            "[1,  9000] loss: 0.719\n",
            "[1, 10000] loss: 0.957\n",
            "[1, 11000] loss: 0.898\n",
            "[1, 12000] loss: 0.816\n",
            "[1, 13000] loss: 0.755\n",
            "[1, 14000] loss: 0.861\n",
            "[1, 15000] loss: 0.636\n",
            "[1, 16000] loss: 0.765\n",
            "[1, 17000] loss: 0.872\n",
            "[1, 18000] loss: 0.862\n",
            "[1, 19000] loss: 0.841\n",
            "[1, 20000] loss: 0.883\n",
            "[1, 21000] loss: 0.860\n",
            "[1, 22000] loss: 0.837\n",
            "[1, 23000] loss: 0.641\n",
            "[1, 24000] loss: 0.737\n",
            "[1, 25000] loss: 0.854\n",
            "[1, 26000] loss: 0.851\n",
            "[1, 27000] loss: 0.766\n",
            "[1, 28000] loss: 0.745\n",
            "[1, 29000] loss: 0.743\n",
            "[1, 30000] loss: 0.835\n",
            "[1, 31000] loss: 0.703\n",
            "[1, 32000] loss: 0.768\n",
            "[1, 33000] loss: 0.788\n",
            "[1, 34000] loss: 0.780\n",
            "[1, 35000] loss: 0.853\n",
            "[1, 36000] loss: 0.747\n",
            "[1, 37000] loss: 1.047\n",
            "[1, 38000] loss: 1.014\n",
            "[1, 39000] loss: 0.695\n",
            "[1, 40000] loss: 0.930\n",
            "[1, 41000] loss: 0.785\n",
            "[1, 42000] loss: 0.845\n",
            "[1, 43000] loss: 0.628\n",
            "[1, 44000] loss: 0.786\n",
            "[1, 45000] loss: 0.907\n",
            "[1, 46000] loss: 0.625\n",
            "[1, 47000] loss: 0.686\n",
            "[1, 48000] loss: 0.874\n",
            "[1, 49000] loss: 0.549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.000003)\n",
        "lr_sche=optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
        "\n",
        "for epoch in range(2):\n",
        "    lr_sche.step()\n",
        "    # training\n",
        "    for batch_idx, (train_inputs, train_labels) in enumerate(train_loader): # train_inputs size:[32,1,28,28], labels size: [32]\n",
        "        train_inputs = train_inputs.to(device)\n",
        "        train_labels = train_labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        WeightOperation.WeightSave()\n",
        "        WeightOperation.WeightBinarize()\n",
        "\n",
        "        predicted = model(train_inputs)\n",
        "        loss = loss_fn(predicted, train_labels)\n",
        "        loss.backward()     # gradient\n",
        "\n",
        "        WeightOperation.WeightRestore()\n",
        "        WeightOperation.WeightGradient()\n",
        "\n",
        "        optimizer.step()    # update\n",
        "      \n",
        "\n",
        "        if((batch_idx*len(train_inputs))%1000==0):\n",
        "           print('[%d, %5d] loss: %.3f' %(epoch, batch_idx*len(train_inputs), loss.item()))  # loss: loss tensor(2.3027, grad_fn=<NllLossBackward>)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkbPSusJUI9n",
        "outputId": "2f1a05ba-f6c1-4001-8a3f-ebb9c06d275c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: UserWarning: Use of index_put_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[indices] = tensor (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:515.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:195: UserWarning: Use of masked_fill_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[mask] = scalar (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:946.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:196: UserWarning: Use of index_put_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[indices] = tensor (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:515.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:196: UserWarning: Use of masked_fill_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[mask] = scalar (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:946.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0,     0] loss: 0.184\n",
            "[0,  1000] loss: 0.239\n",
            "[0,  2000] loss: 0.270\n",
            "[0,  3000] loss: 0.270\n",
            "[0,  4000] loss: 0.229\n",
            "[0,  5000] loss: 0.244\n",
            "[0,  6000] loss: 0.191\n",
            "[0,  7000] loss: 0.291\n",
            "[0,  8000] loss: 0.226\n",
            "[0,  9000] loss: 0.174\n",
            "[0, 10000] loss: 0.164\n",
            "[0, 11000] loss: 0.247\n",
            "[0, 12000] loss: 0.220\n",
            "[0, 13000] loss: 0.252\n",
            "[0, 14000] loss: 0.223\n",
            "[0, 15000] loss: 0.306\n",
            "[0, 16000] loss: 0.142\n",
            "[0, 17000] loss: 0.200\n",
            "[0, 18000] loss: 0.195\n",
            "[0, 19000] loss: 0.240\n",
            "[0, 20000] loss: 0.189\n",
            "[0, 21000] loss: 0.260\n",
            "[0, 22000] loss: 0.225\n",
            "[0, 23000] loss: 0.253\n",
            "[0, 24000] loss: 0.234\n",
            "[0, 25000] loss: 0.185\n",
            "[0, 26000] loss: 0.322\n",
            "[0, 27000] loss: 0.202\n",
            "[0, 28000] loss: 0.174\n",
            "[0, 29000] loss: 0.184\n",
            "[0, 30000] loss: 0.265\n",
            "[0, 31000] loss: 0.278\n",
            "[0, 32000] loss: 0.276\n",
            "[0, 33000] loss: 0.206\n",
            "[0, 34000] loss: 0.210\n",
            "[0, 35000] loss: 0.277\n",
            "[0, 36000] loss: 0.192\n",
            "[0, 37000] loss: 0.196\n",
            "[0, 38000] loss: 0.258\n",
            "[0, 39000] loss: 0.212\n",
            "[0, 40000] loss: 0.261\n",
            "[0, 41000] loss: 0.170\n",
            "[0, 42000] loss: 0.172\n",
            "[0, 43000] loss: 0.202\n",
            "[0, 44000] loss: 0.233\n",
            "[0, 45000] loss: 0.250\n",
            "[0, 46000] loss: 0.269\n",
            "[0, 47000] loss: 0.201\n",
            "[0, 48000] loss: 0.222\n",
            "[0, 49000] loss: 0.226\n",
            "[1,     0] loss: 0.186\n",
            "[1,  1000] loss: 0.172\n",
            "[1,  2000] loss: 0.186\n",
            "[1,  3000] loss: 0.175\n",
            "[1,  4000] loss: 0.317\n",
            "[1,  5000] loss: 0.233\n",
            "[1,  6000] loss: 0.150\n",
            "[1,  7000] loss: 0.285\n",
            "[1,  8000] loss: 0.202\n",
            "[1,  9000] loss: 0.264\n",
            "[1, 10000] loss: 0.114\n",
            "[1, 11000] loss: 0.190\n",
            "[1, 12000] loss: 0.259\n",
            "[1, 13000] loss: 0.194\n",
            "[1, 14000] loss: 0.222\n",
            "[1, 15000] loss: 0.213\n",
            "[1, 16000] loss: 0.237\n",
            "[1, 17000] loss: 0.244\n",
            "[1, 18000] loss: 0.132\n",
            "[1, 19000] loss: 0.223\n",
            "[1, 20000] loss: 0.157\n",
            "[1, 21000] loss: 0.211\n",
            "[1, 22000] loss: 0.241\n",
            "[1, 23000] loss: 0.332\n",
            "[1, 24000] loss: 0.199\n",
            "[1, 25000] loss: 0.168\n",
            "[1, 26000] loss: 0.204\n",
            "[1, 27000] loss: 0.240\n",
            "[1, 28000] loss: 0.216\n",
            "[1, 29000] loss: 0.270\n",
            "[1, 30000] loss: 0.195\n",
            "[1, 31000] loss: 0.264\n",
            "[1, 32000] loss: 0.299\n",
            "[1, 33000] loss: 0.191\n",
            "[1, 34000] loss: 0.230\n",
            "[1, 35000] loss: 0.213\n",
            "[1, 36000] loss: 0.155\n",
            "[1, 37000] loss: 0.282\n",
            "[1, 38000] loss: 0.213\n",
            "[1, 39000] loss: 0.192\n",
            "[1, 40000] loss: 0.207\n",
            "[1, 41000] loss: 0.245\n",
            "[1, 42000] loss: 0.159\n",
            "[1, 43000] loss: 0.211\n",
            "[1, 44000] loss: 0.244\n",
            "[1, 45000] loss: 0.201\n",
            "[1, 46000] loss: 0.254\n",
            "[1, 47000] loss: 0.214\n",
            "[1, 48000] loss: 0.238\n",
            "[1, 49000] loss: 0.357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model,'/content/xnor_con6/model1' )"
      ],
      "metadata": {
        "id": "xKY3jOmU4jBU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # test\n",
        "correct = 0\n",
        "WeightOperation.WeightSave()\n",
        "WeightOperation.WeightBinarize()\n",
        "\n",
        "for (test_inputs, test_labels) in test_loader:\n",
        "    test_input = test_inputs.to('cuda')\n",
        "    test_labels = test_labels.to('cuda')\n",
        "    predicted = model(test_inputs)\n",
        "    pred = predicted.data.max(1, keepdim = False)[1] # max(0):column-wise, max(1):row-wise, [0]:values [1]:index\n",
        "    correct += pred.eq(test_labels.data).sum()\n",
        "\n",
        "    acc = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    WeightOperation.WeightRestore()\n",
        "\n",
        "    print('Accuracy:', acc.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fou45qwlhuda",
        "outputId": "735315b4-cfa6-4d85-89d7-fa0259156d9a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.699999988079071\n",
            "Accuracy: 1.4199999570846558\n",
            "Accuracy: 2.109999895095825\n",
            "Accuracy: 2.7799999713897705\n",
            "Accuracy: 3.4599997997283936\n",
            "Accuracy: 4.069999694824219\n",
            "Accuracy: 4.769999980926514\n",
            "Accuracy: 5.46999979019165\n",
            "Accuracy: 6.199999809265137\n",
            "Accuracy: 6.87999963760376\n",
            "Accuracy: 7.449999809265137\n",
            "Accuracy: 8.109999656677246\n",
            "Accuracy: 8.729999542236328\n",
            "Accuracy: 9.429999351501465\n",
            "Accuracy: 10.109999656677246\n",
            "Accuracy: 10.829999923706055\n",
            "Accuracy: 11.460000038146973\n",
            "Accuracy: 12.1899995803833\n",
            "Accuracy: 12.769999504089355\n",
            "Accuracy: 13.399999618530273\n",
            "Accuracy: 14.089999198913574\n",
            "Accuracy: 14.789999961853027\n",
            "Accuracy: 15.420000076293945\n",
            "Accuracy: 16.149999618530273\n",
            "Accuracy: 16.84000015258789\n",
            "Accuracy: 17.56999969482422\n",
            "Accuracy: 18.170000076293945\n",
            "Accuracy: 18.799999237060547\n",
            "Accuracy: 19.44999885559082\n",
            "Accuracy: 20.1299991607666\n",
            "Accuracy: 20.85999870300293\n",
            "Accuracy: 21.5\n",
            "Accuracy: 22.170000076293945\n",
            "Accuracy: 22.849998474121094\n",
            "Accuracy: 23.5\n",
            "Accuracy: 24.189998626708984\n",
            "Accuracy: 24.81999969482422\n",
            "Accuracy: 25.43000030517578\n",
            "Accuracy: 26.09000015258789\n",
            "Accuracy: 26.649999618530273\n",
            "Accuracy: 27.31999969482422\n",
            "Accuracy: 27.979999542236328\n",
            "Accuracy: 28.619998931884766\n",
            "Accuracy: 29.25\n",
            "Accuracy: 29.869998931884766\n",
            "Accuracy: 30.5\n",
            "Accuracy: 31.170000076293945\n",
            "Accuracy: 31.739999771118164\n",
            "Accuracy: 32.39999771118164\n",
            "Accuracy: 33.189998626708984\n",
            "Accuracy: 33.79999923706055\n",
            "Accuracy: 34.529998779296875\n",
            "Accuracy: 35.23999786376953\n",
            "Accuracy: 35.93000030517578\n",
            "Accuracy: 36.62999725341797\n",
            "Accuracy: 37.31999969482422\n",
            "Accuracy: 38.0\n",
            "Accuracy: 38.57999801635742\n",
            "Accuracy: 39.14999771118164\n",
            "Accuracy: 39.849998474121094\n",
            "Accuracy: 40.5099983215332\n",
            "Accuracy: 41.18000030517578\n",
            "Accuracy: 41.84000015258789\n",
            "Accuracy: 42.47999954223633\n",
            "Accuracy: 43.12999725341797\n",
            "Accuracy: 43.849998474121094\n",
            "Accuracy: 44.39999771118164\n",
            "Accuracy: 45.029998779296875\n",
            "Accuracy: 45.63999938964844\n",
            "Accuracy: 46.29999923706055\n",
            "Accuracy: 47.0\n",
            "Accuracy: 47.62999725341797\n",
            "Accuracy: 48.27000045776367\n",
            "Accuracy: 48.87999725341797\n",
            "Accuracy: 49.53999710083008\n",
            "Accuracy: 50.2599983215332\n",
            "Accuracy: 50.88999938964844\n",
            "Accuracy: 51.6099967956543\n",
            "Accuracy: 52.13999938964844\n",
            "Accuracy: 52.79999923706055\n",
            "Accuracy: 53.5\n",
            "Accuracy: 54.21999740600586\n",
            "Accuracy: 55.0\n",
            "Accuracy: 55.689998626708984\n",
            "Accuracy: 56.32999801635742\n",
            "Accuracy: 56.96999740600586\n",
            "Accuracy: 57.54999923706055\n",
            "Accuracy: 58.29999923706055\n",
            "Accuracy: 58.94999694824219\n",
            "Accuracy: 59.529998779296875\n",
            "Accuracy: 60.21999740600586\n",
            "Accuracy: 60.869998931884766\n",
            "Accuracy: 61.54999923706055\n",
            "Accuracy: 62.23999786376953\n",
            "Accuracy: 62.88999938964844\n",
            "Accuracy: 63.529998779296875\n",
            "Accuracy: 64.22999572753906\n",
            "Accuracy: 64.7699966430664\n",
            "Accuracy: 65.36000061035156\n",
            "Accuracy: 65.95999908447266\n"
          ]
        }
      ]
    }
  ]
}